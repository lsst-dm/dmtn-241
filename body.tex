\section{Introduction}
%This is the LSST overview paper: \cite{2008arXiv0805.2366I}.

\citet{I08_adassxxxii} introduces the core design principals behind the Rubin science processing pipelines. With this philosophy in mind, we have created systems, known as the Butler and Middleware. These systems manage the complexity of storing, retrieving, and organizing large numbers of files and information, such as the relationships between them. These systems work as an abstraction that algorithmic code can sit on top of such that authors only need to concern themselves with performing scientific measurements.

\section{The Butler and Middleware}
The Butler is a system that knows all about data, so a user or code author does not need to. It knows where the data is stored, the format it is stored in, and the relationships between datasets. Users of this system only need to deal with in-memory python objects as the Butler manages appropriate on-disk storage format, as well as formatters that know how to convert between the two. Furthermore, this abstraction allows us to manage transparent conversions to various in-memory data representations for the same underlying on disk format, i.e. PyArrow vs Pandas DataFrames.

The Butler is a high level interface that most users and software make use of. It itself though is made up of two lower-level pieces, the Registry and the Datastore. The Registry is a SQL database which is compatible with several different implementations. It's job is to maintain all the data about the data. This includes unique identifiers, as well as relations between the data sets. The Datastore is what actually stores the datasets. It understand the file formats used to store data and their python in memory equivalent types. The files themselves may be stored on one or more backends, such as; POSIX, S3, Google cloud services, WebDav, or may never hit any disk at all.

We refer to the collection of software that interacts with the Butler as the Middleware. This software handles everything from organizing user queries to orchestrating the processing of data stored within the butler, which will be covered more in-depth later.

A more in-depth description of the Butler and middleware can be found in [cite spie paper].

\section{Datasets}
In order to do it's job of managing datasets, the Butler's view of what a dataset is is more than just a bunch of bits on a disk somewhere. Each dataset tracked by the Butler is a composition of three pieces, a DatasetType, a data ID, and a collection.

The DatasetType describes the "what this is" of a dataset. It has a name to use an an identifier, a storage class that maps on disk bits into in memory python objects, and a set of dimensions. These dimensions are like the set of identifier keys that describe how to address the given DatasetType. More on dimensions later.

A data ID is akin to the address for a given DatasetType. It is a mapping of dimensions, the same dimensions that are present in the DataSetType, to corresponding concrete values. These form a "DataCoordinate" within the set of all dimensions, just like an x,y,z does within a cartesian space.

Collections are a way to group data together, and to label individual datasets. A collection is simply a name, but it allows two or more datasets that may exist at the same "DataCoordinate" to both exist within the Butler without any collision of ambiguity. Though within a given collection there can only be one dataset at a given coordinate.

\section{Dimensions}
As mentioned Dimensions are a way to organize, localize, and identify data within a large volume of possibilities. The set of dimensions is fixed for a given project before any data is tracked. This can be thought of as the "data model" of the project, a complete description of all the identifiers that might be associated with any given dataset.

Dimensions additionally can relate to each other. For instance the Rubin dimension system has dimensions of exposure and detector. Because the instrument has 189 individual detectors, a given exposure implies 189 corresponding points along the detector dimension. Alternatively, it is possible to investigate for a given detector, how does data vary along the exposure dimension (and thus is a proxy for time). Dimensions may relate to each other in logical ways, such as the previous example, as well as spatially, or temporally.

\section{Making use of Data in the Butler}
All algorithms in the Rubin processing pipeline are special objects called Tasks. Tasks are a single logical job, composable inside other tasks, and have a schema for configuration which is separately defined and able to be persisted. All tasks operate on in memory data products, normally supplied from another task.

There is a special subclass of tasks, called PipelineTasks, that get their in memory data by supplying information to the middleware so that it may do I/O with the butler on the tasks behalf. PipelineTasks declare an interface of "connections" which is the tasks desired input / output dataset types, along with the connections dimensions. Tasks also declare a set of dimensions they operate on. The combination of these dimensions with the connection dimensions allows to decide how to supply inputs to the task. For example, extending the example above, if the task declares its dimensions to be exposure, and an input to be exposure, detector then a single invocation of the task will be supplied 189 inputs (one for each detector). In contrast if the task's dimensions are exposure, detector as well, then there will be 189 separate invocations of the task, each receiving one input.

\section{Pipelines}
Pipeline files, written in yaml syntax, provide a way of grouping multiple Pipeline tasks together. In a Pipeline, each PipelineTask is declared with a unique label, and may contain any configuration values that are pertinent to the Pipeline. Pipelines are designed for a purpose, such as preparing calibrations, or processing data from a specific instrument. As such it is possible that a PipelineTask may appear in more that one Pipeline with different configurations. Using labels, it is also possible that the same PipelineTask appear multiple times with unique configurations within one Pipeline. The order in which PipelineTasks are declared withing a pipeline is not important.

The middleware is able to turn these declarative pipelines into directed acyclic graphs (DAG) in execution dependency order. It does this by examining the connections declared by each task in the Pipeline, in the context of it's configuration which may modify said connections. This system insulates task authors from the overall complexity, and provides maximum reusability, as the system is able to use a task in widely varying contexts. It also allows authors to work in a system they are more familiar with from software development where they need only be concerned with interfaces rather than the whole application.

Once the middleware has constructed a PipelineGraph, that graph can be consumed in further processing steps. The PipelineGraph is taken along with a Butler location, user specified collections of data, and user specified data constraints.  The middleware then examines the input nodes in a PipelineGraph to determine the set of data in the butler that matches the node's DataSetType and dimensions. Using these inputs and information the Butler knows about the relations between dimensions, the middleware is able fill in all the remaining nodes based on what it predicts will be created, consumed, and output. Each of these new nodes is called a Quantum, and represents an individual invocation of a task. The collection of all the nodes in the new DAG is known as the QuantumGraph. Creation of this graph is done before any data processing takes place, allowing for appropriate resources to be allocated or scheduled. This same code can be used to process data on a wide range of targets, from a laptop, with 100s of nodes in a QuantumGraph, or scaled up to production runs on compute clusters on physical hardware or in the cloud as outlined in \citet{P52_adassxxxii}.

\section{Conclusions}
The Butler and middleware allows Rubin to abstract all data access from task authors or end users. Abstracting data access also allows allows data processing to be abstracted using PipelineTasks and Pipelines. These abstractions allow more rapid, orthogonal development to take place, as well as providing a mechanism to use the same code base to scale from individual development needs all the way up to running the complete survey. These systems have been successfully deployed in processing Subaru's Hyper-SuprimeGam PDR4 [citation needed], and is being used in NASA's SPHEREx. The code is all open source and available from the Python package index.

\vskip 0.4in